{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, Random Forest, and Adaboost\n",
    "In hw3, you need to implement decision tree, random forest and adaboost by using only numpy, then train your implemented model by the provided dataset and test the performance with testing data\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Package and Import\n",
    "\n",
    "Install the packages which will be used, and import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "feature_names = data['feature_names']\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"x_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "x_test = pd.read_csv(\"x_test.csv\")\n",
    "y_test = pd.read_csv(\"y_test.csv\")\n",
    "\n",
    "y_train = y_train.values[:, 0]\n",
    "y_test = y_test.values[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.423</td>\n",
       "      <td>27.88</td>\n",
       "      <td>59.26</td>\n",
       "      <td>271.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.04971</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>...</td>\n",
       "      <td>10.49</td>\n",
       "      <td>34.24</td>\n",
       "      <td>66.50</td>\n",
       "      <td>330.6</td>\n",
       "      <td>0.1073</td>\n",
       "      <td>0.07158</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0.06969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.070</td>\n",
       "      <td>13.44</td>\n",
       "      <td>77.83</td>\n",
       "      <td>445.2</td>\n",
       "      <td>0.11000</td>\n",
       "      <td>0.09009</td>\n",
       "      <td>0.03781</td>\n",
       "      <td>0.02798</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.06608</td>\n",
       "      <td>...</td>\n",
       "      <td>13.45</td>\n",
       "      <td>15.77</td>\n",
       "      <td>86.92</td>\n",
       "      <td>549.9</td>\n",
       "      <td>0.1521</td>\n",
       "      <td>0.16320</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.07393</td>\n",
       "      <td>0.2781</td>\n",
       "      <td>0.08052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.160</td>\n",
       "      <td>26.60</td>\n",
       "      <td>126.20</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.10200</td>\n",
       "      <td>0.14530</td>\n",
       "      <td>0.19210</td>\n",
       "      <td>0.09664</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.06220</td>\n",
       "      <td>...</td>\n",
       "      <td>23.72</td>\n",
       "      <td>35.90</td>\n",
       "      <td>159.80</td>\n",
       "      <td>1724.0</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>0.38410</td>\n",
       "      <td>0.57540</td>\n",
       "      <td>0.18720</td>\n",
       "      <td>0.3258</td>\n",
       "      <td>0.09720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.000</td>\n",
       "      <td>18.91</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.08217</td>\n",
       "      <td>0.08028</td>\n",
       "      <td>0.09271</td>\n",
       "      <td>0.05627</td>\n",
       "      <td>0.1946</td>\n",
       "      <td>0.05044</td>\n",
       "      <td>...</td>\n",
       "      <td>22.32</td>\n",
       "      <td>25.73</td>\n",
       "      <td>148.20</td>\n",
       "      <td>1538.0</td>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.32070</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.2841</td>\n",
       "      <td>0.06541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.540</td>\n",
       "      <td>19.32</td>\n",
       "      <td>115.10</td>\n",
       "      <td>951.6</td>\n",
       "      <td>0.08968</td>\n",
       "      <td>0.11980</td>\n",
       "      <td>0.10360</td>\n",
       "      <td>0.07488</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>0.05491</td>\n",
       "      <td>...</td>\n",
       "      <td>20.42</td>\n",
       "      <td>25.84</td>\n",
       "      <td>139.50</td>\n",
       "      <td>1239.0</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>0.34200</td>\n",
       "      <td>0.35080</td>\n",
       "      <td>0.19390</td>\n",
       "      <td>0.2928</td>\n",
       "      <td>0.07867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.580</td>\n",
       "      <td>22.14</td>\n",
       "      <td>134.70</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>0.09090</td>\n",
       "      <td>0.13480</td>\n",
       "      <td>0.16400</td>\n",
       "      <td>0.09561</td>\n",
       "      <td>0.1765</td>\n",
       "      <td>0.05024</td>\n",
       "      <td>...</td>\n",
       "      <td>23.24</td>\n",
       "      <td>27.84</td>\n",
       "      <td>158.30</td>\n",
       "      <td>1656.0</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.29200</td>\n",
       "      <td>0.38610</td>\n",
       "      <td>0.19200</td>\n",
       "      <td>0.2909</td>\n",
       "      <td>0.05865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.738</td>\n",
       "      <td>11.97</td>\n",
       "      <td>61.24</td>\n",
       "      <td>288.5</td>\n",
       "      <td>0.09250</td>\n",
       "      <td>0.04102</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1903</td>\n",
       "      <td>0.06422</td>\n",
       "      <td>...</td>\n",
       "      <td>10.62</td>\n",
       "      <td>14.10</td>\n",
       "      <td>66.53</td>\n",
       "      <td>342.9</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.07204</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.3105</td>\n",
       "      <td>0.08151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.270</td>\n",
       "      <td>12.91</td>\n",
       "      <td>98.17</td>\n",
       "      <td>725.5</td>\n",
       "      <td>0.08182</td>\n",
       "      <td>0.06230</td>\n",
       "      <td>0.05892</td>\n",
       "      <td>0.03157</td>\n",
       "      <td>0.1359</td>\n",
       "      <td>0.05526</td>\n",
       "      <td>...</td>\n",
       "      <td>17.38</td>\n",
       "      <td>15.92</td>\n",
       "      <td>113.70</td>\n",
       "      <td>932.7</td>\n",
       "      <td>0.1222</td>\n",
       "      <td>0.21860</td>\n",
       "      <td>0.29620</td>\n",
       "      <td>0.10350</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.07474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.870</td>\n",
       "      <td>16.21</td>\n",
       "      <td>88.52</td>\n",
       "      <td>593.7</td>\n",
       "      <td>0.08743</td>\n",
       "      <td>0.05492</td>\n",
       "      <td>0.01502</td>\n",
       "      <td>0.02088</td>\n",
       "      <td>0.1424</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>15.11</td>\n",
       "      <td>25.58</td>\n",
       "      <td>96.74</td>\n",
       "      <td>694.4</td>\n",
       "      <td>0.1153</td>\n",
       "      <td>0.10080</td>\n",
       "      <td>0.05285</td>\n",
       "      <td>0.05556</td>\n",
       "      <td>0.2362</td>\n",
       "      <td>0.07113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11.710</td>\n",
       "      <td>17.19</td>\n",
       "      <td>74.68</td>\n",
       "      <td>420.3</td>\n",
       "      <td>0.09774</td>\n",
       "      <td>0.06141</td>\n",
       "      <td>0.03809</td>\n",
       "      <td>0.03239</td>\n",
       "      <td>0.1516</td>\n",
       "      <td>0.06095</td>\n",
       "      <td>...</td>\n",
       "      <td>13.01</td>\n",
       "      <td>21.39</td>\n",
       "      <td>84.42</td>\n",
       "      <td>521.5</td>\n",
       "      <td>0.1323</td>\n",
       "      <td>0.10400</td>\n",
       "      <td>0.15210</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.07097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        9.423         27.88           59.26      271.3          0.08123   \n",
       "1       12.070         13.44           77.83      445.2          0.11000   \n",
       "2       19.160         26.60          126.20     1138.0          0.10200   \n",
       "3       19.000         18.91          123.40     1138.0          0.08217   \n",
       "4       17.540         19.32          115.10      951.6          0.08968   \n",
       "5       20.580         22.14          134.70     1290.0          0.09090   \n",
       "6        9.738         11.97           61.24      288.5          0.09250   \n",
       "7       15.270         12.91           98.17      725.5          0.08182   \n",
       "8       13.870         16.21           88.52      593.7          0.08743   \n",
       "9       11.710         17.19           74.68      420.3          0.09774   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.04971         0.00000              0.00000         0.1742   \n",
       "1           0.09009         0.03781              0.02798         0.1657   \n",
       "2           0.14530         0.19210              0.09664         0.1902   \n",
       "3           0.08028         0.09271              0.05627         0.1946   \n",
       "4           0.11980         0.10360              0.07488         0.1506   \n",
       "5           0.13480         0.16400              0.09561         0.1765   \n",
       "6           0.04102         0.00000              0.00000         0.1903   \n",
       "7           0.06230         0.05892              0.03157         0.1359   \n",
       "8           0.05492         0.01502              0.02088         0.1424   \n",
       "9           0.06141         0.03809              0.03239         0.1516   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.06059  ...         10.49          34.24            66.50   \n",
       "1                 0.06608  ...         13.45          15.77            86.92   \n",
       "2                 0.06220  ...         23.72          35.90           159.80   \n",
       "3                 0.05044  ...         22.32          25.73           148.20   \n",
       "4                 0.05491  ...         20.42          25.84           139.50   \n",
       "5                 0.05024  ...         23.24          27.84           158.30   \n",
       "6                 0.06422  ...         10.62          14.10            66.53   \n",
       "7                 0.05526  ...         17.38          15.92           113.70   \n",
       "8                 0.05883  ...         15.11          25.58            96.74   \n",
       "9                 0.06095  ...         13.01          21.39            84.42   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0       330.6            0.1073            0.07158          0.00000   \n",
       "1       549.9            0.1521            0.16320          0.16220   \n",
       "2      1724.0            0.1782            0.38410          0.57540   \n",
       "3      1538.0            0.1021            0.22640          0.32070   \n",
       "4      1239.0            0.1381            0.34200          0.35080   \n",
       "5      1656.0            0.1178            0.29200          0.38610   \n",
       "6       342.9            0.1234            0.07204          0.00000   \n",
       "7       932.7            0.1222            0.21860          0.29620   \n",
       "8       694.4            0.1153            0.10080          0.05285   \n",
       "9       521.5            0.1323            0.10400          0.15210   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0               0.00000          0.2475                  0.06969  \n",
       "1               0.07393          0.2781                  0.08052  \n",
       "2               0.18720          0.3258                  0.09720  \n",
       "3               0.12180          0.2841                  0.06541  \n",
       "4               0.19390          0.2928                  0.07867  \n",
       "5               0.19200          0.2909                  0.05865  \n",
       "6               0.00000          0.3105                  0.08151  \n",
       "7               0.10350          0.2320                  0.07474  \n",
       "8               0.05556          0.2362                  0.07113  \n",
       "9               0.10990          0.2572                  0.07097  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from the course slides on E3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, there are only two labels, but the following code enable data which has more than two labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(sequence):\n",
    "    \n",
    "    c = Counter(sequence) # count the occurence of the labels\n",
    "        \n",
    "    gini = 1\n",
    "    for ci in c.values():\n",
    "        gini -= (ci / sum(c.values())) ** 2\n",
    "\n",
    "    return gini\n",
    "\n",
    "def entropy(sequence):\n",
    "    \n",
    "    c = Counter(sequence) # count the occurence of the labels\n",
    "        \n",
    "    entropy = 0\n",
    "    for ci in c.values():\n",
    "        if not ci == 0:\n",
    "            entropy -= (ci / sum(c.values())) * math.log(ci / sum(c.values()), 2)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1\n",
    "# 2 = class 2\n",
    "data = np.array([1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of the data is 0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(f'Gini of the data is {gini(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the data is 0.9456603046006402\n"
     ]
    }
   ],
   "source": [
    "print(f'Entropy of the data is {entropy(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the test data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **Criterion**: The function to measure the quality of a split. Your model should support “gini” for the Gini impurity and “entropy” for the information gain. \n",
    "2. **Max_depth**: The maximum depth of the tree. If Max_depth=None, then nodes are expanded until all leaves are pure. Max_depth=1 equals to split data once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Node Class\n",
    "\n",
    "A class of tree node will be defined in the following.\n",
    "\n",
    "A decision tree will be built by tree nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, data, labels, depth):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.depth = depth  # node depth\n",
    "        self.impurity = math.inf\n",
    "        self.feature = None\n",
    "        self.threshold = None\n",
    "        self.left = None # left child node\n",
    "        self.right = None # right child node\n",
    "        return None\n",
    "    \n",
    "    # set multiple attributes\n",
    "    def set_attributes(self, feature, threshold, left, right):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Class\n",
    "\n",
    "A class of decision tree will be defined in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        if criterion == 'gini':\n",
    "            self.criterion = gini\n",
    "        else:\n",
    "            self.criterion = entropy\n",
    "        self.root = None  # the root of the tree\n",
    "        self.max_depth = max_depth\n",
    "        self.num_feats = None  # number of features in training data\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.root = Node(np.array(X), np.array(y), 0)\n",
    "        self.num_feats = np.array(X).shape[1]\n",
    "        self.partition(self.root)  # split the tree by features and threshold in the root node\n",
    "        return None\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(np.array(X))\n",
    "        return accuracy_score(y_pred, np.array(y))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for x in np.array(X):\n",
    "            node = self.root\n",
    "            # when the left child and right child is None, it reach the leaf\n",
    "            while not node.left == None and not node.right == None:\n",
    "                if x[node.feature] < node.threshold:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            # choose the most common label in the node\n",
    "            y_pred.append(np.argmax(np.bincount(node.labels)))\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def partition(self, node):\n",
    "        # when the node reach the maximum depth, or all the labels in the node are the same then stop splitting\n",
    "        if (self.max_depth != None and node.depth == self.max_depth) or len(np.unique(node.labels)) == 1:\n",
    "            return None\n",
    "        \n",
    "        # search features\n",
    "        for feat in range(self.num_feats):\n",
    "            # search threshold\n",
    "            for t in ((np.insert(np.sort(node.data[:, feat]), 0, 0) + np.insert(np.sort(node.data[:, feat]), len(node.data[:, feat]), np.sort(node.data[:, feat])[-1])) / 2)[1:-1]:\n",
    "                # partition data\n",
    "                left_part = node.data[node.data[:, feat] < t]\n",
    "                left_labels = node.labels[node.data[:, feat] < t]\n",
    "                right_part = node.data[node.data[:, feat] >= t]\n",
    "                right_labels = node.labels[node.data[:, feat] >= t]\n",
    "                # determine whether the partition is better\n",
    "                tmp_impurity = (len(left_labels) / (len(left_labels) + len(right_labels))) * self.criterion(left_labels) + (len(right_labels) / (len(left_labels) + len(right_labels))) * self.criterion(right_labels)\n",
    "                if tmp_impurity < node.impurity:\n",
    "                    node.impurity = tmp_impurity\n",
    "                    left_tree = Node(np.array(left_part), np.array(left_labels), node.depth + 1)\n",
    "                    right_tree = Node(np.array(right_part), np.array(right_labels), node.depth + 1)\n",
    "                    node.set_attributes(feat, t, left_tree, right_tree)\n",
    "        # after partition, if the data is still not pure, partition its children\n",
    "        if node.impurity != 0:\n",
    "            self.partition(node.left)\n",
    "            self.partition(node.right)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using Criterion=‘gini’, showing the accuracy score of test data by Max_depth=3 and Max_depth=10, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "\n",
    "clf_depth3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the testing data using the decision tree which its maximum depth is 3 is 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy of the testing data using the decision tree which its maximum depth is 3 is {clf_depth3.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
    "\n",
    "clf_depth10.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the testing data using the decision tree which its maximum depth is 10 is 0.916083916083916\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy of the testing data using the decision tree which its maximum depth is 10 is {clf_depth10.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using Max_depth=3, showing the accuracy score of test data by Criterion=‘gini’ and Criterion=’entropy’, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "\n",
    "clf_gini.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the testing data using the decision tree which its criterion is gini is 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy of the testing data using the decision tree which its criterion is gini is {clf_gini.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
    "\n",
    "clf_entropy.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the testing data using the decision tree which its criterion is entropy is 0.9370629370629371\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy of the testing data using the decision tree which its criterion is entropy is {clf_entropy.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the feature counts for building tree without normalize the importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = dict(enumerate(feature_names))\n",
    "\n",
    "feats = []\n",
    "\n",
    "def get_feats(node):\n",
    "    # when the node's left child and right child are both null, it is the leaf node\n",
    "    if node.left == None and node.right == None:\n",
    "        return None\n",
    "    feats.append(node.feature)\n",
    "    get_feats(node.left)\n",
    "    get_feats(node.right)\n",
    "    return None\n",
    "\n",
    "get_feats(clf_depth10.root)\n",
    "\n",
    "feats_counter = Counter(sorted(feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = []\n",
    "y_labels = []\n",
    "\n",
    "for (key, value) in feats_counter.most_common()[-1::-1]:\n",
    "    y_labels.append(feat_dict[key])\n",
    "    width.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = np.arange(len(width))\n",
    "plt.barh(y, width)\n",
    "plt.yticks(y, y_labels)\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Occurrence')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **N_estimators**: The number of trees in the forest. \n",
    "2. **Max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **Bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features, bootstrap=True, criterion='gini', max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = int(max_features)\n",
    "        self.bootstrap = bootstrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.clfs = []  # store the estimators\n",
    "        self.feats_used = []  # store features which are used\n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        # train numbers of estimators\n",
    "        for _ in range(self.n_estimators):\n",
    "            # random select data with replacement\n",
    "            rand_data_idx = np.random.choice(X.shape[0], size=X.shape[0], replace=True)\n",
    "            # random select features with replacement\n",
    "            rand_feat_idx = np.random.choice(X.shape[1], size=self.max_features, replace=False)\n",
    "            self.feats_used.append(rand_feat_idx)\n",
    "            rand_data = X[rand_data_idx, :][:, rand_feat_idx]\n",
    "            rand_label = y[rand_data_idx]\n",
    "            clf = DecisionTree(criterion=self.criterion, max_depth=self.max_depth)\n",
    "            clf.fit(rand_data, rand_label)\n",
    "            self.clfs.append(clf)\n",
    "        return None\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # get predictions of each classifier\n",
    "        votes = []\n",
    "        for (clf, feats) in zip(self.clfs, self.feats_used):\n",
    "            votes.append(clf.predict(np.array(X)[:, feats]))\n",
    "        votes = np.array(votes)\n",
    "        # vote the predictions of each classifier\n",
    "        preds = []\n",
    "        for j in range(votes.shape[1]):\n",
    "            counts = np.bincount(votes[:, j])\n",
    "            preds.append(np.argmax(counts))\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(np.array(X))\n",
    "        return accuracy_score(y_pred, np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Using Criterion=‘gini’, Max_depth=None, Max_features=sqrt(n_features), showing the accuracy score of test data by n_estimators=10 and n_estimators=100, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "\n",
    "clf_10tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the testing data using random forest with 10 estimators is 0.9370629370629371\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy of the testing data using random forest with 10 estimators is {clf_10tree.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(x_train.shape[1]))\n",
    "\n",
    "clf_100tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the testing data using random forest with 100 estimators is 0.958041958041958\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy of the testing data using random forest with 100 estimators is {clf_100tree.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2\n",
    "Using Criterion=‘gini’, Max_depth=None, N_estimators=10, showing the accuracy score of test data by Max_features=sqrt(n_features) and Max_features=n_features, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "\n",
    "clf_random_features.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the testing data using random forest with only a few features is 0.9370629370629371\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy of the testing data using random forest with only a few features is {clf_random_features.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_all_features = RandomForest(n_estimators=10, max_features=x_train.shape[1])\n",
    "\n",
    "clf_all_features.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the testing data using random forest with all features is 0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy of the testing data using random forest with all features is {clf_all_features.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
